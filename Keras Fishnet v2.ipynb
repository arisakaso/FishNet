{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.keras import TqdmCallback\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = 224\n",
    "batch_size=16      #32 from paper but getting OOM with 32\n",
    "epochs=50          #100 from paper \n",
    "weight_decay= 0.0001\n",
    "lr=0.1  \n",
    "momentum=0.9\n",
    "number_classes=10 #1000 for Imagenet\n",
    "\n",
    "traindir = 'C:\\\\Imagenet\\\\train\\\\2\\\\'    #10 classes, 1298 per class\n",
    "valdir = 'C:\\\\Imagenet\\\\test\\\\2\\\\'       #10 classes, 2 images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12634 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "#train_datagen = ImageDataGenerator(  \n",
    "#    rescale=1./255,\n",
    "#    horizontal_flip=True)\n",
    "\n",
    "train_it = train_datagen.flow_from_directory(traindir, target_size=(input_size, input_size), shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "val_it = val_datagen.flow_from_directory(valdir, target_size=(input_size, input_size), shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck_JQ (layers.Layer):\n",
    "    \"\"\" \n",
    "    This class replicates the Bottleneck block of the FishNet modle\n",
    "    \n",
    "    This layer class takes in the concatenated outputs in the FishNet of the SAME DIMENSIONS and reduces the number of channels. \n",
    "\n",
    "    There are two paths through this layer. The first is a series of convolutions, the second, optional path, is a channel reduciton funciton. The output from each path are added together to arrive at the final output\n",
    "      \n",
    "    inplanes is the number of input channels, planes is the number of output channels. In between the convolutions use bottleneck_channels, which are 1/4 of the number of output channels\n",
    "      \n",
    "    inplanes is implemented in pytorch but it is not necessary in keras. the iput channels is given by the previous layer (i think)\n",
    "\n",
    "    The 'squeeze_idt' function has been renamed as the 'channel_reduction' although it can also increase the number of channels. \n",
    "\n",
    "    k needs to be set as an integer. According to the paper, k is supposed to be c_in//c_out but this is something we need to double check.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, planes, stride=1, mode = 'NORM', k=1, dilation=1):\n",
    "        \n",
    "        super(Bottleneck_JQ, self).__init__()\n",
    "     \n",
    "        self.stride = stride\n",
    "        self.mode = mode\n",
    "        self.k = k\n",
    "        self.planes = planes\n",
    "      \n",
    "        bottleneck_channels = planes // 4 ##the number of channels in the bottlenect is 1/4 of the output channels. Need to read the paper to understand why\n",
    "     \n",
    "        self.relu = layers.Activation('relu') #same relu activation settings is used in all parts of this class, so it is only defined once.\n",
    "\n",
    "        self.bn1 =  layers.BatchNormalization()\n",
    "        self.conv1 = layers.Conv2D(filters = bottleneck_channels, kernel_size = 1, activation=None, padding = \"same\", use_bias=False)\n",
    "\n",
    "        self.bn2 = layers.BatchNormalization() #this is unecessary because the input channels are already specified by the previous layer in keras, but i'm keeping it for symetry with the original code\n",
    "        self.conv2 = layers.Conv2D(filters = bottleneck_channels, kernel_size = 3, strides = stride, use_bias=False, activation=None, padding = \"same\", dilation_rate = dilation)\n",
    "\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.conv3 = layers.Conv2D(filters = planes, kernel_size = 1, use_bias=False, activation=None, padding = \"same\", dilation_rate = dilation)\n",
    "\n",
    "        if mode == 'UP':\n",
    "            self.shortcut = None\n",
    "        elif inplanes != planes or stride > 1:  #i don't know how to apply the inplanes != planes logic here... its not something that would be relevant to keras, but i guess we can also just forcibly specify it. UPDATE: that's exactly what i did, but its not elegant. \n",
    "            self.shortcut = keras.Sequential()\n",
    "            self.shortcut.add(layers.BatchNormalization())\n",
    "            self.shortcut.add(layers.Activation('relu'))\n",
    "            self.shortcut.add(layers.Conv2D(filters = planes, kernel_size = 1, strides = stride, use_bias=False, activation=None, padding = \"same\", dilation_rate = dilation))\n",
    "        else: \n",
    "            self.shortcut = None #if the inplanes equal the planes, ie no change in number of channels, then we just don't implement this bypass step\n",
    "\n",
    "    def _pre_act_forward(self, x):\n",
    "        \n",
    "        residual = x\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.mode == 'UP':\n",
    "            residual = self.channel_reduction(x)\n",
    "        elif self.shortcut is not None:\n",
    "            residual = self.shortcut(residual)\n",
    "\n",
    "        out = layers.add([out, residual])\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def channel_reduction(self,idt):\n",
    "        n = tf.shape(idt)[0]\n",
    "        h = tf.shape(idt)[1]\n",
    "        w = tf.shape(idt)[2]\n",
    "        c = tf.shape(idt)[3]\n",
    "        \n",
    "        idt_rehsaped = tf.reshape(idt, (n, h, w, c // self.k, self.k))\n",
    "        idt_reduced = tf.math.reduce_sum(idt_rehsaped, axis=-1)\n",
    "        return idt_reduced  # this should be correct and line 274 should be modified.\n",
    "      \n",
    "      # n, h, w, c = tf.shape(idt)\n",
    "      ## the above fails because these can be undefined when building the computational graph.\n",
    "      ## see https://github.com/tensorflow/models/issues/6245\n",
    "      # k = self.k\n",
    "      # planes = self.planes\n",
    "      # store = tf.math.reduce_sum(idt[:,:,:,0:k],axis=3, keepdims=True)\n",
    "      # for i in range(1,planes):\n",
    "      #   print(store.shape)\n",
    "      #   store = layers.concatenate([store,tf.math.reduce_sum(idt[:,:,:,i*k:(i*k)+k],axis=3, keepdims=True)])\n",
    "      # return store\n",
    "      \n",
    "\n",
    "    def call(self, x):\n",
    "        out = self._pre_act_forward(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test_mdl1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_612 (Conv2D)             (None, 112, 112, 32) 864         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 112, 112, 32) 128         conv2d_612[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, 112, 112, 32) 0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_613 (Conv2D)             (None, 112, 112, 32) 9216        activation_260[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 112, 112, 32) 128         conv2d_613[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, 112, 112, 32) 0           batch_normalization_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_614 (Conv2D)             (None, 112, 112, 64) 18432       activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_610 (BatchN (None, 112, 112, 64) 256         conv2d_614[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, 112, 112, 64) 0           batch_normalization_610[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling2D) (None, 56, 56, 64)   0           activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_175 (Bottleneck_J (None, 56, 56, 128)  24320       max_pooling2d_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_176 (Bottleneck_J (None, 56, 56, 128)  18176       bottleneck_jq_175[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D) (None, 28, 28, 128)  0           bottleneck_jq_176[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_177 (Bottleneck_J (None, 28, 28, 256)  95744       max_pooling2d_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_178 (Bottleneck_J (None, 28, 28, 256)  71168       bottleneck_jq_177[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_179 (Bottleneck_J (None, 28, 28, 256)  71168       bottleneck_jq_178[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_180 (Bottleneck_J (None, 28, 28, 256)  71168       bottleneck_jq_179[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling2D) (None, 14, 14, 256)  0           bottleneck_jq_180[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_181 (Bottleneck_J (None, 14, 14, 512)  379904      max_pooling2d_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_182 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_181[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_183 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_182[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_184 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_183[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_185 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_184[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_186 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_185[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_187 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_186[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_188 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_187[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling2D) (None, 7, 7, 512)    0           bottleneck_jq_188[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 7, 7, 512)    2048        max_pooling2d_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_280 (Activation)     (None, 7, 7, 512)    0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 7, 7, 256)    131072      activation_280[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 7, 7, 256)    1024        conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 7, 7, 1024)   262144      batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 7, 7, 1024)   4096        conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_281 (Activation)     (None, 7, 7, 1024)   0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 1024)         0           activation_281[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 1, 1, 1024)   0           global_average_pooling2d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 1, 1, 32)     32768       reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_282 (Activation)     (None, 1, 1, 32)     0           conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 1, 1, 512)    16384       activation_282[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, 1, 1, 512)    0           conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling2D) (None, 7, 7, 512)    0           activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_189 (Bottleneck_J (None, 7, 7, 512)    281600      up_sampling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_190 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_189[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_191 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_190[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_192 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_191[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_193 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_192[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_194 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_193[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling2D) (None, 14, 14, 512)  0           bottleneck_jq_194[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_195 (Bottleneck_J (None, 14, 14, 256)  221696      up_sampling2d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_196 (Bottleneck_J (None, 14, 14, 256)  71168       bottleneck_jq_195[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_197 (Bottleneck_J (None, 14, 14, 384)  245504      bottleneck_jq_196[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_198 (Bottleneck_J (None, 14, 14, 384)  158976      bottleneck_jq_197[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_18 (UpSampling2D) (None, 28, 28, 384)  0           bottleneck_jq_198[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_199 (Bottleneck_J (None, 28, 28, 128)  78080       up_sampling2d_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_200 (Bottleneck_J (None, 28, 28, 128)  18176       bottleneck_jq_199[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_201 (Bottleneck_J (None, 28, 28, 256)  95744       bottleneck_jq_200[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_202 (Bottleneck_J (None, 28, 28, 256)  71168       bottleneck_jq_201[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_19 (UpSampling2D) (None, 56, 56, 256)  0           bottleneck_jq_202[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_203 (Bottleneck_J (None, 56, 56, 64)   25984       up_sampling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_204 (Bottleneck_J (None, 56, 56, 64)   4736        bottleneck_jq_203[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_205 (Bottleneck_J (None, 56, 56, 320)  109952      bottleneck_jq_204[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_206 (Bottleneck_J (None, 56, 56, 320)  110720      bottleneck_jq_205[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling2D) (None, 28, 28, 320)  0           bottleneck_jq_206[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_207 (Bottleneck_J (None, 28, 28, 512)  421376      max_pooling2d_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_208 (Bottleneck_J (None, 28, 28, 512)  281600      bottleneck_jq_207[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_209 (Bottleneck_J (None, 28, 28, 832)  1100672     bottleneck_jq_208[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_210 (Bottleneck_J (None, 28, 28, 832)  740480      bottleneck_jq_209[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D) (None, 14, 14, 832)  0           bottleneck_jq_210[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_211 (Bottleneck_J (None, 14, 14, 768)  1286144     max_pooling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_212 (Bottleneck_J (None, 14, 14, 768)  631296      bottleneck_jq_211[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_213 (Bottleneck_J (None, 14, 14, 1600) 3625344     bottleneck_jq_212[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_214 (Bottleneck_J (None, 14, 14, 1600) 2729600     bottleneck_jq_213[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_215 (Bottleneck_J (None, 14, 14, 1600) 2729600     bottleneck_jq_214[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_216 (Bottleneck_J (None, 14, 14, 1600) 2729600     bottleneck_jq_215[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D) (None, 7, 7, 1600)   0           bottleneck_jq_216[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_217 (Bottleneck_J (None, 7, 7, 512)    1250816     max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_218 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_217[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_219 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_218[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_220 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_219[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 7, 7, 2112)   0           max_pooling2d_34[0][0]           \n",
      "                                                                 bottleneck_jq_220[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_766 (BatchN (None, 7, 7, 2112)   8448        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 7, 7, 2112)   0           batch_normalization_766[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_771 (Conv2D)             (None, 7, 7, 1056)   2230272     activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_767 (BatchN (None, 7, 7, 1056)   4224        conv2d_771[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 1056)         0           batch_normalization_767[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 1, 1056)   0           global_average_pooling2d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_772 (Conv2D)             (None, 1, 1, 10)     10560       reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           conv2d_772[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 26,707,744\n",
      "Trainable params: 26,614,496\n",
      "Non-trainable params: 93,248\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "#Start of keras model\n",
    "x1 = layers.Conv2D(32, 3, activation=None, padding = \"same\", strides=(2,2), use_bias=False)(img_inputs)\n",
    "x2 = layers.BatchNormalization()(x1) \n",
    "x3 = layers.Activation('relu')(x2)\n",
    "\n",
    "x4 = layers.Conv2D(32, 3, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x3)\n",
    "x5 = layers.BatchNormalization()(x4) \n",
    "x6 = layers.Activation('relu')(x5)\n",
    "\n",
    "x7 = layers.Conv2D(64, 3, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x6)\n",
    "x8 = layers.BatchNormalization()(x7) \n",
    "x9 = layers.Activation('relu')(x8)\n",
    "\n",
    "x10 = layers.MaxPool2D(2, strides = 2)(x9) \n",
    "\n",
    "#56x56 stage\n",
    "x11 = Bottleneck_JQ(64,128)(x10) #NORM type bottleneck layer, shortcut is used.\n",
    "x12 = Bottleneck_JQ(128,128)(x11) #NORM type but inplanes = planes so there will be no shortcut\n",
    "\n",
    "x13 = layers.MaxPool2D(2, strides = 2)(x12)\n",
    "\n",
    "#28X28 Stage\n",
    "x14 = Bottleneck_JQ(128,256)(x13)\n",
    "x15 = Bottleneck_JQ(256,256)(x14)\n",
    "x16 = Bottleneck_JQ(256,256)(x15)\n",
    "x17 = Bottleneck_JQ(256,256)(x16)\n",
    "\n",
    "x17a = layers.MaxPool2D(2, strides = 2)(x17)\n",
    "\n",
    "#14X14 Stage\n",
    "x18 = Bottleneck_JQ(256,512)(x17a)\n",
    "x19 = Bottleneck_JQ(512,512)(x18)\n",
    "x20 = Bottleneck_JQ(512,512)(x19)\n",
    "x21 = Bottleneck_JQ(512,512)(x20)\n",
    "x22 = Bottleneck_JQ(512,512)(x21)\n",
    "x23 = Bottleneck_JQ(512,512)(x22)\n",
    "x24 = Bottleneck_JQ(512,512)(x23)\n",
    "x25 = Bottleneck_JQ(512,512)(x24)\n",
    "\n",
    "x25a = layers.MaxPool2D(2, strides = 2)(x25)\n",
    "\n",
    "#7X7 Stage\n",
    "x26 = layers.BatchNormalization()(x25a) \n",
    "x27 = layers.Activation('relu')(x26)\n",
    "x28 = layers.Conv2D(256, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x27)\n",
    "x29 = layers.BatchNormalization()(x28) \n",
    "x30 = layers.Conv2D(1024, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x29)\n",
    "x31 = layers.BatchNormalization()(x30) \n",
    "x32 = layers.Activation('relu')(x31) \n",
    "\n",
    "#1X1 Stage\n",
    "x33 = layers.GlobalAveragePooling2D()(x32)\n",
    "x33a = layers.Reshape((1,1,1024))(x33)\n",
    "x34 = layers.Conv2D(32, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x33a)  #sq_conv\n",
    "x35 = layers.Activation('relu')(x34)\n",
    "x36 = layers.Conv2D(512, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x35) #ex_conv\n",
    "x37 = layers.Activation('sigmoid')(x36)\n",
    "\n",
    "x37a = layers.UpSampling2D((7,7))(x37)\n",
    "\n",
    "#7X7 Stage\n",
    "x38 = Bottleneck_JQ(512,512)(x37a)\n",
    "x39 = Bottleneck_JQ(512,512)(x38)\n",
    "x40 = Bottleneck_JQ(512,512)(x39)\n",
    "x41 = Bottleneck_JQ(512,512)(x40)\n",
    "x42 = Bottleneck_JQ(512,512)(x41)\n",
    "x43 = Bottleneck_JQ(512,512)(x42)\n",
    "\n",
    "#BODY\n",
    "#14x14 upsampling\n",
    "x44 = layers.UpSampling2D((2,2))(x43)\n",
    "\n",
    "#14X14 Stage\n",
    "x45 = Bottleneck_JQ(512,256)(x44)\n",
    "x46 = Bottleneck_JQ(256,256)(x45)\n",
    "x47 = Bottleneck_JQ(256,384)(x46)\n",
    "x48 = Bottleneck_JQ(384,384)(x47)\n",
    "\n",
    "x48a = layers.UpSampling2D((2,2))(x48)\n",
    "\n",
    "#28X28 Stage\n",
    "x49 = Bottleneck_JQ(384,128)(x48a)\n",
    "x50 = Bottleneck_JQ(128,128)(x49)\n",
    "x51 = Bottleneck_JQ(128,256)(x50)\n",
    "x52 = Bottleneck_JQ(256,256)(x51)\n",
    "\n",
    "x52a = layers.UpSampling2D((2,2))(x52)\n",
    "\n",
    "#56X56 Stage\n",
    "x53 = Bottleneck_JQ(256,64)(x52a)\n",
    "x54 = Bottleneck_JQ(64,64)(x53)\n",
    "x55 = Bottleneck_JQ(64,320)(x54)\n",
    "x56 = Bottleneck_JQ(320,320)(x55)\n",
    "\n",
    "x56a = layers.MaxPool2D(2, strides = 2)(x56)\n",
    "\n",
    "#HEAD\n",
    "#28X28 Stage\n",
    "x57 = Bottleneck_JQ(320,512)(x56a)\n",
    "x58 = Bottleneck_JQ(512,512)(x57)\n",
    "x59 = Bottleneck_JQ(512,832)(x58)\n",
    "x60 = Bottleneck_JQ(832,832)(x59)\n",
    "\n",
    "x60a = layers.MaxPool2D(2, strides = 2)(x60)\n",
    "\n",
    "#14X14 Stage\n",
    "x61 = Bottleneck_JQ(832,768)(x60a)\n",
    "x62 = Bottleneck_JQ(768,768)(x61)\n",
    "x63 = Bottleneck_JQ(768,1600)(x62)\n",
    "x64 = Bottleneck_JQ(1600,1600)(x63)\n",
    "x65 = Bottleneck_JQ(1600,1600)(x64)\n",
    "x66 = Bottleneck_JQ(1600,1600)(x65)\n",
    "\n",
    "x66a = layers.MaxPool2D(2, strides = 2)(x66)\n",
    "\n",
    "#7X7 Stage\n",
    "x67 = Bottleneck_JQ(1600,512)(x66a)\n",
    "x68 = Bottleneck_JQ(512,512)(x67)\n",
    "x69 = Bottleneck_JQ(512,512)(x68)\n",
    "x70 = Bottleneck_JQ(512,512)(x69)\n",
    "\n",
    "x71 = layers.Concatenate()([x66a,x70])\n",
    "\n",
    "x72 = layers.BatchNormalization()(x71)\n",
    "x73 = layers.Activation('relu')(x72)\n",
    "x74 = layers.Conv2D(1056, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x73)\n",
    "x75 = layers.BatchNormalization()(x74)\n",
    "x76 = layers.GlobalAveragePooling2D()(x75) #does not translate from pytorch perfectly\n",
    "x77 = layers.Reshape((1,1,1056))(x76)\n",
    "x78 = layers.Conv2D(number_classes, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x77)\n",
    "\n",
    "# output layers (these aren't part of FishNet, this is our own top layer to make it fit the dataset we are using)\n",
    "img_outputs2 = layers.Flatten()(x78)\n",
    "test_mdl1 = keras.Model(img_inputs, img_outputs2, name=\"test_mdl1\")\n",
    "test_mdl1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(learning_rate=lr, momentum=momentum)  # TODO:weight decay\n",
    "test_mdl1.compile(\n",
    "    optimizer=opt,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8303c38bcb48919ca6d37d9ae2c632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a5ddb45e304c8d9b9343fc42fbc729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/790 [..............................] - ETA: 5:09:48 - loss: 2.2128 - accuracy: 0.3125"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "history = test_mdl1.fit(\n",
    "    x=train_it,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_it,\n",
    "    callbacks=[TqdmCallback(verbose=1)],\n",
    ")\n",
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "hist.plot(y=[\"loss\", \"val_loss\"], ax=ax[0])\n",
    "hist.plot(y=[\"accuracy\", \"val_accuracy\"], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
