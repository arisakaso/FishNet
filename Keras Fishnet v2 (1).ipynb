{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.keras import TqdmCallback\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_size = 224\n",
    "batch_size=16      #32 from paper but getting OOM with 32\n",
    "epochs=50          #100 from paper \n",
    "weight_decay= 0.0001\n",
    "lr=0.1  \n",
    "momentum=0.9\n",
    "number_classes=10 #1000 for Imagenet\n",
    "\n",
    "traindir = 'C:\\\\Imagenet\\\\train\\\\2\\\\'    #10 classes, 1298 per class\n",
    "valdir = 'C:\\\\Imagenet\\\\test\\\\2\\\\'       #10 classes, 2 images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12634 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "#train_datagen = ImageDataGenerator(  \n",
    "#    rescale=1./255,\n",
    "#    horizontal_flip=True)\n",
    "\n",
    "train_it = train_datagen.flow_from_directory(traindir, target_size=(input_size, input_size), shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255)\n",
    "\n",
    "val_it = val_datagen.flow_from_directory(valdir, target_size=(input_size, input_size), shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck_JQ (layers.Layer):\n",
    "    \"\"\" \n",
    "    This class replicates the Bottleneck block of the FishNet modle\n",
    "    \n",
    "    This layer class takes in the concatenated outputs in the FishNet of the SAME DIMENSIONS and reduces the number of channels. \n",
    "\n",
    "    There are two paths through this layer. The first is a series of convolutions, the second, optional path, is a channel reduciton funciton. The output from each path are added together to arrive at the final output\n",
    "      \n",
    "    inplanes is the number of input channels, planes is the number of output channels. In between the convolutions use bottleneck_channels, which are 1/4 of the number of output channels\n",
    "      \n",
    "    inplanes is implemented in pytorch but it is not necessary in keras. the iput channels is given by the previous layer (i think)\n",
    "\n",
    "    The 'squeeze_idt' function has been renamed as the 'channel_reduction' although it can also increase the number of channels. \n",
    "\n",
    "    k needs to be set as an integer. According to the paper, k is supposed to be c_in//c_out but this is something we need to double check.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes, planes, stride=1, mode = 'NORM', k=1, dilation=1):\n",
    "        \n",
    "        super(Bottleneck_JQ, self).__init__()\n",
    "     \n",
    "        self.stride = stride\n",
    "        self.mode = mode\n",
    "        self.k = k\n",
    "        self.planes = planes\n",
    "      \n",
    "        bottleneck_channels = planes // 4 ##the number of channels in the bottlenect is 1/4 of the output channels. Need to read the paper to understand why\n",
    "     \n",
    "        self.relu = layers.Activation('relu') #same relu activation settings is used in all parts of this class, so it is only defined once.\n",
    "\n",
    "        self.bn1 =  layers.BatchNormalization()\n",
    "        self.conv1 = layers.Conv2D(filters = bottleneck_channels, kernel_size = 1, activation=None, padding = \"same\", use_bias=False)\n",
    "\n",
    "        self.bn2 = layers.BatchNormalization() #this is unecessary because the input channels are already specified by the previous layer in keras, but i'm keeping it for symetry with the original code\n",
    "        self.conv2 = layers.Conv2D(filters = bottleneck_channels, kernel_size = 3, strides = stride, use_bias=False, activation=None, padding = \"same\", dilation_rate = dilation)\n",
    "\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.conv3 = layers.Conv2D(filters = planes, kernel_size = 1, use_bias=False, activation=None, padding = \"same\", dilation_rate = dilation)\n",
    "\n",
    "        if mode == 'UP':\n",
    "            self.shortcut = None\n",
    "        elif inplanes != planes or stride > 1:  #i don't know how to apply the inplanes != planes logic here... its not something that would be relevant to keras, but i guess we can also just forcibly specify it. UPDATE: that's exactly what i did, but its not elegant. \n",
    "            self.shortcut = keras.Sequential()\n",
    "            self.shortcut.add(layers.BatchNormalization())\n",
    "            self.shortcut.add(layers.Activation('relu'))\n",
    "            self.shortcut.add(layers.Conv2D(filters = planes, kernel_size = 1, strides = stride, use_bias=False, activation=None, padding = \"same\", dilation_rate = dilation))\n",
    "        else: \n",
    "            self.shortcut = None #if the inplanes equal the planes, ie no change in number of channels, then we just don't implement this bypass step\n",
    "\n",
    "    def _pre_act_forward(self, x):\n",
    "        \n",
    "        residual = x\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if self.mode == 'UP':\n",
    "            residual = self.channel_reduction(x)\n",
    "        elif self.shortcut is not None:\n",
    "            residual = self.shortcut(residual)\n",
    "\n",
    "        out = layers.add([out, residual])\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def channel_reduction(self,idt):\n",
    "        n = tf.shape(idt)[0]\n",
    "        h = tf.shape(idt)[1]\n",
    "        w = tf.shape(idt)[2]\n",
    "        c = tf.shape(idt)[3]\n",
    "        \n",
    "        idt_rehsaped = tf.reshape(idt, (n, h, w, c // self.k, self.k))\n",
    "        idt_reduced = tf.math.reduce_sum(idt_rehsaped, axis=-1)\n",
    "        return idt_reduced  # this should be correct and line 274 should be modified.\n",
    "      \n",
    "      # n, h, w, c = tf.shape(idt)\n",
    "      ## the above fails because these can be undefined when building the computational graph.\n",
    "      ## see https://github.com/tensorflow/models/issues/6245\n",
    "      # k = self.k\n",
    "      # planes = self.planes\n",
    "      # store = tf.math.reduce_sum(idt[:,:,:,0:k],axis=3, keepdims=True)\n",
    "      # for i in range(1,planes):\n",
    "      #   print(store.shape)\n",
    "      #   store = layers.concatenate([store,tf.math.reduce_sum(idt[:,:,:,i*k:(i*k)+k],axis=3, keepdims=True)])\n",
    "      # return store\n",
    "      \n",
    "\n",
    "    def call(self, x):\n",
    "        out = self._pre_act_forward(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: \"test_mdl1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1546 (Conv2D)            (None, 112, 112, 32) 864         input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1536 (Batch (None, 112, 112, 32) 128         conv2d_1546[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_651 (Activation)     (None, 112, 112, 32) 0           batch_normalization_1536[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1547 (Conv2D)            (None, 112, 112, 32) 9216        activation_651[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1537 (Batch (None, 112, 112, 32) 128         conv2d_1547[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_652 (Activation)     (None, 112, 112, 32) 0           batch_normalization_1537[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1548 (Conv2D)            (None, 112, 112, 64) 18432       activation_652[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1538 (Batch (None, 112, 112, 64) 256         conv2d_1548[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_653 (Activation)     (None, 112, 112, 64) 0           batch_normalization_1538[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_69 (MaxPooling2D) (None, 56, 56, 64)   0           activation_653[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_444 (Bottleneck_J (None, 56, 56, 128)  24320       max_pooling2d_69[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_445 (Bottleneck_J (None, 56, 56, 128)  18176       bottleneck_jq_444[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling2D) (None, 28, 28, 128)  0           bottleneck_jq_445[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_446 (Bottleneck_J (None, 28, 28, 256)  95744       max_pooling2d_70[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_447 (Bottleneck_J (None, 28, 28, 256)  71168       bottleneck_jq_446[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_448 (Bottleneck_J (None, 28, 28, 256)  71168       bottleneck_jq_447[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_449 (Bottleneck_J (None, 28, 28, 256)  71168       bottleneck_jq_448[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_71 (MaxPooling2D) (None, 14, 14, 256)  0           bottleneck_jq_449[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_450 (Bottleneck_J (None, 14, 14, 512)  379904      max_pooling2d_71[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_451 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_450[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_452 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_451[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_453 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_452[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_454 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_453[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_455 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_454[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_456 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_455[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_457 (Bottleneck_J (None, 14, 14, 512)  281600      bottleneck_jq_456[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_72 (MaxPooling2D) (None, 7, 7, 512)    0           bottleneck_jq_457[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1584 (Batch (None, 7, 7, 512)    2048        max_pooling2d_72[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_671 (Activation)     (None, 7, 7, 512)    0           batch_normalization_1584[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1594 (Conv2D)            (None, 7, 7, 256)    131072      activation_671[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1585 (Batch (None, 7, 7, 256)    1024        conv2d_1594[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1595 (Conv2D)            (None, 7, 7, 1024)   262144      batch_normalization_1585[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1586 (Batch (None, 7, 7, 1024)   4096        conv2d_1595[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_672 (Activation)     (None, 7, 7, 1024)   0           batch_normalization_1586[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_17 (Gl (None, 1024)         0           activation_672[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_17 (Reshape)            (None, 1, 1, 1024)   0           global_average_pooling2d_17[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1596 (Conv2D)            (None, 1, 1, 32)     32768       reshape_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_673 (Activation)     (None, 1, 1, 32)     0           conv2d_1596[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1597 (Conv2D)            (None, 1, 1, 512)    16384       activation_673[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_674 (Activation)     (None, 1, 1, 512)    0           conv2d_1597[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_40 (UpSampling2D) (None, 7, 7, 512)    0           activation_674[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_458 (Bottleneck_J (None, 7, 7, 512)    281600      up_sampling2d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_459 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_458[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_460 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_459[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_461 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_460[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_462 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_461[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_463 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_462[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_41 (UpSampling2D) (None, 14, 14, 512)  0           bottleneck_jq_463[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_464 (Bottleneck_J (None, 14, 14, 256)  221696      up_sampling2d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_465 (Bottleneck_J (None, 14, 14, 256)  71168       bottleneck_jq_464[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 14, 14, 768)  0           up_sampling2d_41[0][0]           \n",
      "                                                                 bottleneck_jq_465[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_466 (Bottleneck_J (None, 14, 14, 384)  197376      concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_467 (Bottleneck_J (None, 14, 14, 384)  158976      bottleneck_jq_466[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_42 (UpSampling2D) (None, 28, 28, 384)  0           bottleneck_jq_467[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_468 (Bottleneck_J (None, 28, 28, 128)  78080       up_sampling2d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_469 (Bottleneck_J (None, 28, 28, 128)  18176       bottleneck_jq_468[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 28, 28, 512)  0           up_sampling2d_42[0][0]           \n",
      "                                                                 bottleneck_jq_469[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_470 (Bottleneck_J (None, 28, 28, 256)  88576       concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_471 (Bottleneck_J (None, 28, 28, 256)  71168       bottleneck_jq_470[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_43 (UpSampling2D) (None, 56, 56, 256)  0           bottleneck_jq_471[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_472 (Bottleneck_J (None, 56, 56, 64)   25984       up_sampling2d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_473 (Bottleneck_J (None, 56, 56, 64)   4736        bottleneck_jq_472[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 56, 56, 320)  0           up_sampling2d_43[0][0]           \n",
      "                                                                 bottleneck_jq_473[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_474 (Bottleneck_J (None, 56, 56, 320)  214400      concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_475 (Bottleneck_J (None, 56, 56, 320)  110720      bottleneck_jq_474[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling2D) (None, 28, 28, 320)  0           bottleneck_jq_475[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_476 (Bottleneck_J (None, 28, 28, 512)  421376      max_pooling2d_73[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_477 (Bottleneck_J (None, 28, 28, 512)  281600      bottleneck_jq_476[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 28, 28, 832)  0           max_pooling2d_73[0][0]           \n",
      "                                                                 bottleneck_jq_477[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_478 (Bottleneck_J (None, 28, 28, 832)  1436032     concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_479 (Bottleneck_J (None, 28, 28, 832)  740480      bottleneck_jq_478[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling2D) (None, 14, 14, 832)  0           bottleneck_jq_479[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_480 (Bottleneck_J (None, 14, 14, 768)  1286144     max_pooling2d_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_481 (Bottleneck_J (None, 14, 14, 768)  631296      bottleneck_jq_480[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 14, 14, 1600) 0           max_pooling2d_74[0][0]           \n",
      "                                                                 bottleneck_jq_481[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_482 (Bottleneck_J (None, 14, 14, 1600) 2729600     concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_483 (Bottleneck_J (None, 14, 14, 1600) 2729600     bottleneck_jq_482[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_484 (Bottleneck_J (None, 14, 14, 1600) 2729600     bottleneck_jq_483[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling2D) (None, 7, 7, 1600)   0           bottleneck_jq_484[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_485 (Bottleneck_J (None, 7, 7, 512)    1250816     max_pooling2d_75[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_486 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_485[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_487 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_486[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_jq_488 (Bottleneck_J (None, 7, 7, 512)    281600      bottleneck_jq_487[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 7, 7, 2112)   0           max_pooling2d_75[0][0]           \n",
      "                                                                 bottleneck_jq_488[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1688 (Batch (None, 7, 7, 2112)   8448        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_714 (Activation)     (None, 7, 7, 2112)   0           batch_normalization_1688[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1699 (Conv2D)            (None, 7, 7, 1056)   2230272     activation_714[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1689 (Batch (None, 7, 7, 1056)   4224        conv2d_1699[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_18 (Gl (None, 1056)         0           batch_normalization_1689[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_18 (Reshape)            (None, 1, 1, 1056)   0           global_average_pooling2d_18[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1700 (Conv2D)            (None, 1, 1, 10)     10560       reshape_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 10)           0           conv2d_1700[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 23,466,912\n",
      "Trainable params: 23,375,008\n",
      "Non-trainable params: 91,904\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "#Start of keras model\n",
    "x1 = layers.Conv2D(32, 3, activation=None, padding = \"same\", strides=(2,2), use_bias=False)(img_inputs)\n",
    "x2 = layers.BatchNormalization()(x1) \n",
    "x3 = layers.Activation('relu')(x2)\n",
    "\n",
    "x4 = layers.Conv2D(32, 3, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x3)\n",
    "x5 = layers.BatchNormalization()(x4) \n",
    "x6 = layers.Activation('relu')(x5)\n",
    "\n",
    "x7 = layers.Conv2D(64, 3, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x6)\n",
    "x8 = layers.BatchNormalization()(x7) \n",
    "x9 = layers.Activation('relu')(x8)\n",
    "\n",
    "x10 = layers.MaxPool2D(2, strides = 2)(x9) \n",
    "\n",
    "#56x56 stage\n",
    "x11 = Bottleneck_JQ(64,128)(x10) #NORM type bottleneck layer, shortcut is used.\n",
    "x12 = Bottleneck_JQ(128,128)(x11) #NORM type but inplanes = planes so there will be no shortcut\n",
    "\n",
    "x13 = layers.MaxPool2D(2, strides = 2)(x12)\n",
    "\n",
    "#28X28 Stage\n",
    "x14 = Bottleneck_JQ(128,256)(x13)\n",
    "x15 = Bottleneck_JQ(256,256)(x14)\n",
    "x16 = Bottleneck_JQ(256,256)(x15)\n",
    "x17 = Bottleneck_JQ(256,256)(x16)\n",
    "\n",
    "x17a = layers.MaxPool2D(2, strides = 2)(x17)\n",
    "\n",
    "#14X14 Stage\n",
    "x18 = Bottleneck_JQ(256,512)(x17a)\n",
    "x19 = Bottleneck_JQ(512,512)(x18)\n",
    "x20 = Bottleneck_JQ(512,512)(x19)\n",
    "x21 = Bottleneck_JQ(512,512)(x20)\n",
    "x22 = Bottleneck_JQ(512,512)(x21)\n",
    "x23 = Bottleneck_JQ(512,512)(x22)\n",
    "x24 = Bottleneck_JQ(512,512)(x23)\n",
    "x25 = Bottleneck_JQ(512,512)(x24)\n",
    "\n",
    "x25a = layers.MaxPool2D(2, strides = 2)(x25)\n",
    "\n",
    "#7X7 Stage\n",
    "x26 = layers.BatchNormalization()(x25a) \n",
    "x27 = layers.Activation('relu')(x26)\n",
    "x28 = layers.Conv2D(256, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x27)\n",
    "x29 = layers.BatchNormalization()(x28) \n",
    "x30 = layers.Conv2D(1024, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x29)\n",
    "x31 = layers.BatchNormalization()(x30) \n",
    "x32 = layers.Activation('relu')(x31) \n",
    "\n",
    "#1X1 Stage\n",
    "x33 = layers.GlobalAveragePooling2D()(x32)\n",
    "x33a = layers.Reshape((1,1,1024))(x33)\n",
    "x34 = layers.Conv2D(32, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x33a)  #sq_conv\n",
    "x35 = layers.Activation('relu')(x34)\n",
    "x36 = layers.Conv2D(512, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x35) #ex_conv\n",
    "x37 = layers.Activation('sigmoid')(x36)\n",
    "\n",
    "x37a = layers.UpSampling2D((7,7))(x37)\n",
    "\n",
    "#7X7 Stage\n",
    "x38 = Bottleneck_JQ(512,512)(x37a)\n",
    "x39 = Bottleneck_JQ(512,512)(x38)\n",
    "x40 = Bottleneck_JQ(512,512)(x39)\n",
    "x41 = Bottleneck_JQ(512,512)(x40)\n",
    "x42 = Bottleneck_JQ(512,512)(x41)\n",
    "x43 = Bottleneck_JQ(512,512)(x42)\n",
    "\n",
    "#BODY\n",
    "#14x14 upsampling\n",
    "x44 = layers.UpSampling2D((2,2))(x43)\n",
    "\n",
    "#14X14 Stage\n",
    "x45 = Bottleneck_JQ(512,256)(x44)\n",
    "x46 = Bottleneck_JQ(256,256)(x45)\n",
    "x46a = layers.Concatenate()([x44,x46]) #concatenate\n",
    "#x47 = Bottleneck_JQ(256,384)(x48)\n",
    "x47 = Bottleneck_JQ(768,384, mode='UP', k=2)(x46a)\n",
    "x48 = Bottleneck_JQ(384,384)(x47)\n",
    "\n",
    "x48a = layers.UpSampling2D((2,2))(x48)\n",
    "\n",
    "#28X28 Stage\n",
    "x49 = Bottleneck_JQ(384,128)(x48a)\n",
    "x50 = Bottleneck_JQ(128,128)(x49)\n",
    "x50a = layers.Concatenate()([x48a,x50]) #concatenate\n",
    "x51 = Bottleneck_JQ(512,256, mode = 'UP', k=2)(x50a)\n",
    "#x51 = Bottleneck_JQ(128,256)(x50)\n",
    "x52 = Bottleneck_JQ(256,256)(x51)\n",
    "\n",
    "x52a = layers.UpSampling2D((2,2))(x52)\n",
    "\n",
    "#56X56 Stage\n",
    "x53 = Bottleneck_JQ(256,64)(x52a)\n",
    "x54 = Bottleneck_JQ(64,64)(x53)\n",
    "x54a = layers.Concatenate()([x52a,x54]) #concatenate\n",
    "x55 = Bottleneck_JQ(64,320)(x54a)\n",
    "x56 = Bottleneck_JQ(320,320)(x55)\n",
    "\n",
    "x56a = layers.MaxPool2D(2, strides = 2)(x56)\n",
    "\n",
    "#HEAD\n",
    "#28X28 Stage\n",
    "x57 = Bottleneck_JQ(320,512)(x56a)\n",
    "x58 = Bottleneck_JQ(512,512)(x57)\n",
    "x58a = layers.Concatenate()([x56a,x58]) #concatenate\n",
    "x59 = Bottleneck_JQ(512,832)(x58a)\n",
    "x60 = Bottleneck_JQ(832,832)(x59)\n",
    "\n",
    "x60a = layers.MaxPool2D(2, strides = 2)(x60)\n",
    "\n",
    "#14X14 Stage\n",
    "x61 = Bottleneck_JQ(832,768)(x60a)\n",
    "x62 = Bottleneck_JQ(768,768)(x61)\n",
    "#x63 = Bottleneck_JQ(768,1600)(x62)\n",
    "x63a = layers.Concatenate()([x60a,x62])  #concatenate\n",
    "x64 = Bottleneck_JQ(1600,1600)(x63a)\n",
    "x65 = Bottleneck_JQ(1600,1600)(x64)\n",
    "x66 = Bottleneck_JQ(1600,1600)(x65)\n",
    "\n",
    "x66a = layers.MaxPool2D(2, strides = 2)(x66)\n",
    "\n",
    "#7X7 Stage\n",
    "x67 = Bottleneck_JQ(1600,512)(x66a)\n",
    "x68 = Bottleneck_JQ(512,512)(x67)\n",
    "x69 = Bottleneck_JQ(512,512)(x68)\n",
    "x70 = Bottleneck_JQ(512,512)(x69)\n",
    "\n",
    "x71 = layers.Concatenate()([x66a,x70])\n",
    "\n",
    "x72 = layers.BatchNormalization()(x71)\n",
    "x73 = layers.Activation('relu')(x72)\n",
    "x74 = layers.Conv2D(1056, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x73)\n",
    "x75 = layers.BatchNormalization()(x74)\n",
    "x76 = layers.GlobalAveragePooling2D()(x75) #does not translate from pytorch perfectly\n",
    "x77 = layers.Reshape((1,1,1056))(x76)\n",
    "x78 = layers.Conv2D(number_classes, 1, activation=None, padding = \"same\", strides=(1,1), use_bias=False)(x77)\n",
    "\n",
    "# output layers (these aren't part of FishNet, this is our own top layer to make it fit the dataset we are using)\n",
    "img_outputs2 = layers.Flatten()(x78)\n",
    "test_mdl1 = keras.Model(img_inputs, img_outputs2, name=\"test_mdl1\")\n",
    "test_mdl1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(learning_rate=lr, momentum=momentum)  # TODO:weight decay\n",
    "test_mdl1.compile(\n",
    "    optimizer=opt,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578ae044c8a34428b7ca9f12f371836c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca75ddfb3e2487f910718867de428ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 18/790 [..............................] - ETA: 2:20:28 - loss: 8.2399 - accuracy: 0.1675"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "history = test_mdl1.fit(\n",
    "    x=train_it,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_it,\n",
    "    callbacks=[TqdmCallback(verbose=1)],\n",
    ")\n",
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "hist.plot(y=[\"loss\", \"val_loss\"], ax=ax[0])\n",
    "hist.plot(y=[\"accuracy\", \"val_accuracy\"], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
